{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how I built the database.\n",
    "\n",
    "Download the data from https://github.com/artsmia/collection and unzip it here.  That makes a directory called `collection-main` that has an '`object` subdirectory with a json file for each object. They're arranged in numbered sub-sub-directories, so we'll need to get all of those. Because of how the json files a structured, `pandas` didn't do what I wanted.  So I fell back on the `json` library. I also didn't want to keep every single column (the `see_also` was causing problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# columns to keep\n",
    "keys = ['accession_number', 'artist', 'life_date', 'title', 'classification', 'department', 'continent', 'country', 'culture',\n",
    "        'creditline', 'dated', 'description', 'dimensions', 'medium', 'style', 'text', \n",
    "        'markings', 'room']\n",
    "\n",
    "\n",
    "# And a function to process the files, only keeping the requested columns\n",
    "def get_data(filename):\n",
    "    with open(filename) as f:\n",
    "        temp = json.load(f)\n",
    "    return {k: temp.get(k) for k in keys}\n",
    "\n",
    "# The json parse \n",
    "def is_non_empty(filename):\n",
    "    return os.stat(filename).st_size > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seemed simplest to use `pandas` to convert it into a sql table here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = [get_data(x) for x in glob.glob('collection-main/objects/*/*.json') if is_non_empty(x)]\n",
    "df = pd.DataFrame(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "\n",
    "engine = sqlalchemy.create_engine('postgresql://rich:testpass@localhost:5432/art')\n",
    "connection = engine.connect()\n",
    "\n",
    "df.to_sql('mia', connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're going to do this yourself, you may find it easier to work in `sqlite` instead of setting up a `postgres` server on your laptop. If you do, you might need to adjust the `%s` we used to protect against SQL injection to a `?` (though it depends on the versions of everything).\n",
    "\n",
    "Also, if you see the `pandas` above and think that you can use that to _read_ from the SQL database, you're correct.  However, it's a bit dangerous - if the table is too large, `pandas` will try to pull it down anyway and possibly crash your kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
